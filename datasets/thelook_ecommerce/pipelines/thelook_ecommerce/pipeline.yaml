# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:

  - type: bigquery_table
    table_id: products
    description: "The Look fictitious e-commerce dataset - products table"

  - type: bigquery_table
    table_id: events
    description: "Programatically generated web events for The Look fictitious e-commerce store"

  - type: bigquery_table
    table_id: users
    description: "Programatically generated users for The Look fictitious e-commerce store"

  - type: bigquery_table
    table_id: orders
    description: "Programatically generated orders for The Look fictitious e-commerce store"

  - type: bigquery_table
    table_id: order_items
    description: "Programatically generated order items for The Look fictitious e-commerce store"

  - type: bigquery_table
    table_id: inventory_items
    description: "Programatically generated inventory for The Look fictitious e-commerce store"

  - type: bigquery_table
    table_id: distribution_centers
    description: "The Look fictitious e-commerce dataset: distribution_centers table"

dag:
  airflow_version: 2
  initialize:
    dag_id: thelook_ecommerce
    default_args:
      owner: "Google"

      # When set to True, keeps a task from getting triggered if the previous schedule for the task hasnâ€™t succeeded
      depends_on_past: False
      start_date: '2021-02-09'
    max_active_runs: 1
    schedule_interval: "@daily"  # runs everyday at 6am EST
    catchup: False
    default_view: graph

  tasks:
    - operator: "KubernetesPodOperator"

      # Task description
      description: "Run CSV transform within kubernetes pod"

      args:

        task_id: "generate_thelook"
        is_delete_operator_pod: False
        # The name of the pod in which the task will run. This will be used (plus a random suffix) to generate a pod id
        name: "generate_thelook"
        namespace: "composer"
        service_account_name: "datasets"
        image_pull_policy: "Always"

        # Docker images will be built and pushed to GCR by default whenever the `scripts/generate_dag.py` is run. To skip building and pushing images, use the optional `--skip-builds` flag.
        image: "{{ var.json.thelook_ecommerce.docker_image }}"

        # Set the environment variables you need initialized in the container. Use these as input variables for the script your container is expected to perform.
        env_vars:
          NUM_OF_USERS: "100000"
          NUM_OF_GHOST_EVENTS: "5"
          TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          TARGET_GCS_PREFIX: "data/thelook_ecommerce"
          SOURCE_DIR: "data"
          EXTRANEOUS_HEADERS: "[\"event_type\", \"ip_address\", \"browser\", \"traffic_source\", \"session_id\", \"sequence_number\", \"uri\", \"is_sold\"]"

        resources:
          request_memory: "8G"
          request_cpu: "2"
          request_ephemeral_storage: "10G"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load Products data to a BigQuery table"

      args:
        task_id: "load_products_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/thelook_ecommerce/products.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "thelook_ecommerce.products"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: "id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "cost"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "category"
            type: "STRING"
            mode: "NULLABLE"
          - name: "name"
            type: "STRING"
            mode: "NULLABLE"
          - name: "brand"
            type: "STRING"
            mode: "NULLABLE"
          - name: "retail_price"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "department"
            type: "STRING"
            mode: "NULLABLE"
          - name: "sku"
            type: "STRING"
            mode: "NULLABLE"
          - name: "distribution_center_id"
            type: "INTEGER"
            mode: "NULLABLE"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load Events data to a BigQuery table"

      args:
        task_id: "load_events_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/thelook_ecommerce/events.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "thelook_ecommerce.events"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: "id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "user_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "sequence_number"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "session_id"
            type: "STRING"
            mode: "NULLABLE"
          - name: "created_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "ip_address"
            type: "STRING"
            mode: "NULLABLE"
          - name: "city"
            type: "STRING"
            mode: "NULLABLE"
          - name: "state"
            type: "STRING"
            mode: "NULLABLE"
          - name: "postal_code"
            type: "STRING"
            mode: "NULLABLE"
          - name: "browser"
            type: "STRING"
            mode: "NULLABLE"
          - name: "traffic_source"
            type: "STRING"
            mode: "NULLABLE"
          - name: "uri"
            type: "STRING"
            mode: "NULLABLE"
          - name: "event_type"
            type: "STRING"
            mode: "NULLABLE"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load Iventory Items data to a BigQuery table"

      args:
        task_id: "load_inventory_items_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/thelook_ecommerce/inventory_items.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "thelook_ecommerce.inventory_items"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: "id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "product_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "created_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "sold_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "cost"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "product_category"
            type: "STRING"
            mode: "NULLABLE"
          - name: "product_name"
            type: "STRING"
            mode: "NULLABLE"
          - name: "product_brand"
            type: "STRING"
            mode: "NULLABLE"
          - name: "product_retail_price"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "product_department"
            type: "STRING"
            mode: "NULLABLE"
          - name: "product_sku"
            type: "STRING"
            mode: "NULLABLE"
          - name: "product_distribution_center_id"
            type: "INTEGER"
            mode: "NULLABLE"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load Order Items data to a BigQuery table"

      args:
        task_id: "load_order_items_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/thelook_ecommerce/order_items.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "thelook_ecommerce.order_items"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: "id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "order_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "user_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "product_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "inventory_item_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "status"
            type: "STRING"
            mode: "NULLABLE"
          - name: "created_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "shipped_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "delivered_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "returned_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "sale_price"
            type: "FLOAT"
            mode: "NULLABLE"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load Orders data to a BigQuery table"

      args:
        task_id: "load_orders_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/thelook_ecommerce/orders.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "thelook_ecommerce.orders"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: "order_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "user_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "status"
            type: "STRING"
            mode: "NULLABLE"
          - name: "gender"
            type: "STRING"
            mode: "NULLABLE"
          - name: "created_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "returned_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "shipped_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "delivered_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"
          - name: "num_of_item"
            type: "INTEGER"
            mode: "NULLABLE"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load Users data to a BigQuery table"

      args:
        task_id: "load_users_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/thelook_ecommerce/users.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "thelook_ecommerce.users"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: "id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "first_name"
            type: "STRING"
            mode: "NULLABLE"
          - name: "last_name"
            type: "STRING"
            mode: "NULLABLE"
          - name: "email"
            type: "STRING"
            mode: "NULLABLE"
          - name: "age"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "gender"
            type: "STRING"
            mode: "NULLABLE"
          - name: "state"
            type: "STRING"
            mode: "NULLABLE"
          - name: "street_address"
            type: "STRING"
            mode: "NULLABLE"
          - name: "postal_code"
            type: "STRING"
            mode: "NULLABLE"
          - name: "city"
            type: "STRING"
            mode: "NULLABLE"
          - name: "country"
            type: "STRING"
            mode: "NULLABLE"
          - name: "latitude"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "longitude"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "traffic_source"
            type: "STRING"
            mode: "NULLABLE"
          - name: "created_at"
            type: "TIMESTAMP"
            mode: "NULLABLE"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load Distribution Centers data to a BigQuery table"

      args:
        task_id: "load_distribution_centers_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/thelook_ecommerce/distribution_centers.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "thelook_ecommerce.distribution_centers"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: "id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "name"
            type: "STRING"
            mode: "NULLABLE"
          - name: "latitude"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "longitude"
            type: "FLOAT"
            mode: "NULLABLE"

  graph_paths:
    - "generate_thelook >> [load_products_to_bq, load_events_to_bq, load_inventory_items_to_bq, load_order_items_to_bq, load_orders_to_bq, load_users_to_bq, load_distribution_centers_to_bq]"
