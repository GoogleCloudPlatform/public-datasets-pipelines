# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:
  - type: bigquery_table
    table_id: "hourly_access"
    description: "U.S. Climate Normals Hourly Data (Most Recent)."
  - type: bigquery_table
    table_id: "hourly_historical"
    description: "U.S. Climate Normals Hourly Data (Historical)."

dag:
  airflow_version: 2
  initialize:
    dag_id: normals_hourly
    default_args:
      owner: "Google"
      depends_on_past: False
      start_date: '2021-03-01'
    max_active_runs: 1
    schedule_interval: "0 */6 * * *"
    catchup: False
    default_view: graph

  tasks:
    - operator: "BashOperator"
      description: "Task to copy over to pod, the source data and structure from GCS"
      args:
        task_id: "download_raw_csv_file"
        bash_command: |
          mkdir -p ./files/schema
          mkdir -p ./files/normals-hourly
          gsutil cp -rm gs://normals/normals-hourly * ./files/normals-hourly
    - operator: "GoogleCloudStorageToBigQueryOperator"
      args:
        task_id: "hourly_load_current_AQW"
        bucket: "normals"
        source_objects: [ "normals-hourly/access/AQW*.csv" ]
        destination_project_dataset_table: 'us_climate_normals.normals_hourly_AQW'
        schema_object: 'gs://{{ var.value.composer_bucket }}/data/us_climate_normals/schema/normals_hourly_schema.json'
        # schema_update_options: [ "ALLOW_FIELD_ADDITION" ]
        source_format: 'CSV'
        skip_leading_rows: 1
        create_disposition: 'CREATE_IF_NEEDED'
        write_disposition: 'WRITE_TRUNCATE'
      resources:
        request_memory: "12G"
        request_cpu: "1"
        request_ephemeral_storage: "16G"
    # - operator: "BigQueryInsertJobOperator"
    #   args:
    #     task_id: "convert_datatypes"
    #     configuration:
    #       query:
    #         query: "FOR record in
    #                 (SELECT column_name
    #                   FROM `bigquery-public-data-dev.us_climate_normals.INFORMATION_SCHEMA.COLUMNS`
    #                 WHERE table_name = 'normals_hourly_AQW'
    #                   AND data_type IN ( 'INTEGER', 'INT64' )
    #                 LIMIT 1000)
    #                 DO
    #                   EXECUTE IMMEDIATE format('''
    #                     ALTER TABLE us_climate_normals.normals_hourly_AQW ALTER COLUMN %s SET DATA TYPE FLOAT64;
    #                     ''', record.column_name);
    #                 END FOR;
    #                 "
    #         use_legacy_sql: True
    - operator: "GoogleCloudStorageToBigQueryOperator"
      args:
        task_id: "hourly_load_historical_AQW"
        bucket: "normals"
        source_objects: [ "normals-hourly/1981-2010/access/AQW*.csv", "normals-hourly/1991-2010/access/AQW*.csv", "normals-hourly/2006-2020/access/AQW*.csv" ]
        destination_project_dataset_table: 'us_climate_normals.normals_hourly_AQW'
        # schema_object='gs://test-bucket/schema.json'
        schema_update_options: [ "ALLOW_FIELD_ADDITION" ]
        source_format: 'CSV'
        skip_leading_rows: 1
        create_disposition: 'CREATE_IF_NEEDED'
        write_disposition: 'WRITE_APPEND'
      resources:
        request_memory: "12G"
        request_cpu: "1"
        request_ephemeral_storage: "16G"
  graph_paths:
    - "hourly_load_current_AQW >> hourly_load_historical_AQW"
    # - "hourly_load_current_AQW >> convert_datatypes >> hourly_load_historical_AQW"
    # - "create_cluster >> hourly_load >> delete_cluster"
