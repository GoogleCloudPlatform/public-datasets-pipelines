# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:
  - type: bigquery_table
    table_id: "hourly_access"
    description: "U.S. Climate Normals Hourly Data (Most Recent)."

dag:
  airflow_version: 2
  initialize:
    dag_id: us_climate_normals
    default_args:
      owner: "Google"
      depends_on_past: False
      start_date: '2021-03-01'
    max_active_runs: 1
    schedule_interval: "0 * * * *"
    catchup: False
    default_view: graph

  tasks:
    - operator: "GKECreateClusterOperator"
      args:
        task_id: "create_cluster"
        project_id: "{{ var.value.gcp_project }}"
        location: "us-central1-c"
        body:
          name: us-climate-normals
          initial_node_count: 2
          network: "{{ var.value.vpc_network }}"
          node_config:
            machine_type: e2-standard-4
            oauth_scopes:
              - https://www.googleapis.com/auth/devstorage.read_write
              - https://www.googleapis.com/auth/cloud-platform
    - operator: "GoogleCloudStorageToGoogleCloudStorageOperator"
      description: "Copy hourly (current) source files to public bucket"
      args:
        task_id: copy_hourly_current_files_to_public_bucket
        source_bucket: "normals"
        source_object: "normals-hourly/access/*.csv"
        destination_bucket: "{{ var.value.composer_bucket }}"
        destination_object: "data/us_climate_normals/hourly_access/"
        move_object: False
    - operator: "GoogleCloudStorageToGoogleCloudStorageOperator"
      description: "Copy hourly (historical) source files to public bucket"
      args:
        task_id: copy_hourly_historical_to_public_bucket
        source_bucket: "normals"
        source_object: "normals-hourly/1981/*.csv"
        destination_bucket: "{{ var.value.composer_bucket }}"
        destination_object: "data/us_climate_normals/hourly_access/"
        move_object: False
    - operator: "GKEStartPodOperator"
      description: "Run CSV transform within kubernetes pod"
      args:
        task_id: "us_climate_normals"
        startup_timeout_seconds: 600
        name: "load_us_climate_normals"
        namespace: "default"
        project_id: "{{ var.value.gcp_project }}"
        location: "us-central1-c"
        cluster_name: us-climate-normals
        image_pull_policy: "Always"
        image: "{{ var.json.new_york_taxi_trips.container_registry.run_csv_transform_kub }}"
        env_vars:
          SOURCE_BUCKET: "{{ var.json.new_york_taxi_trips.container_registry.green_trips_source_url }}"
          # SOURCE_FILE: "files/data_green_trips.csv"
          # TARGET_FILE: "files/data_output_green_trips.csv"
          # PROJECT_ID: "{{ var.value.gcp_project }}"
          # DATASET_ID: "{{ var.json.new_york_taxi_trips.container_registry.green_trips_dataset_id }}"
          # TABLE_ID: "{{ var.json.new_york_taxi_trips.container_registry.green_trips_table_id }}"
          # DATA_FILE_YEAR_FIELD: "data_file_year"
          # DATA_FILE_MONTH_FIELD: "data_file_month"
          # SCHEMA_PATH: "{{ var.json.new_york_taxi_trips.container_registry.green_trips_schema_path }}"
          # CHUNKSIZE: "{{ var.json.new_york_taxi_trips.container_registry.green_trips_chunk_size }}"
          # TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          # TARGET_GCS_PATH: "{{ var.json.new_york_taxi_trips.container_registry.green_trips_target_gcs_path }}"
          # PIPELINE_NAME: "tlc_green_trips"
          # INPUT_CSV_HEADERS: >-
          #   ["vendor_id", "pickup_datetime", "dropoff_datetime", "store_and_fwd_flag", "rate_code",
          #    "pickup_location_id", "dropoff_location_id", "passenger_count", "trip_distance", "fare_amount",
          #    "extra", "mta_tax", "tip_amount", "tolls_amount", "ehail_fee",
          #    "imp_surcharge", "total_amount", "payment_type", "trip_type", "congestion_surcharge" ]
          # DATA_DTYPES: >-
          #   { "vendor_id": "str",
          #     "pickup_datetime": "datetime64[ns]",
          #     "dropoff_datetime": "datetime64[ns]",
          #     "store_and_fwd_flag": "str",
          #     "rate_code": "str",
          #     "pickup_location_id": "str",
          #     "dropoff_location_id": "str",
          #     "passenger_count": "str",
          #     "trip_distance": "float64",
          #     "fare_amount": "float64",
          #     "extra": "float64",
          #     "mta_tax": "float64",
          #     "tip_amount": "float64",
          #     "tolls_amount": "float64",
          #     "ehail_fee": "float64",
          #     "imp_surcharge": "float64",
          #     "total_amount": "float64",
          #     "payment_type": "str",
          #     "trip_type": "str",
          #     "congestion_surcharge": "float64" }
          # OUTPUT_CSV_HEADERS: >-
          #   [ "vendor_id", "pickup_datetime", "dropoff_datetime", "store_and_fwd_flag", "rate_code",
          #     "passenger_count", "trip_distance", "fare_amount", "extra", "mta_tax",
          #     "tip_amount", "tolls_amount", "ehail_fee", "total_amount", "payment_type",
          #     "distance_between_service", "time_between_service", "trip_type", "imp_surcharge", "pickup_location_id",
          #     "dropoff_location_id", "data_file_year", "data_file_month" ]
        resources:
          request_memory: "12G"
          request_cpu: "1"
          request_ephemeral_storage: "16G"
    - operator: "GKEDeleteClusterOperator"
      args:
        task_id: "delete_cluster"
        project_id: "{{ var.value.gcp_project }}"
        location: "us-central1-c"
        name: us-climate-normals

  graph_paths:
    - "copy_source_files_to_public_bucket >> hourly_access"
