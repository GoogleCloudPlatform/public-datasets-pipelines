# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:
  - type: bigquery_table
    table_id: recovered_cases
    description: "Recovered_cases Dataset"

dag:
  airflow_version: 2
  initialize:
    dag_id: recovered_cases
    default_args:
      owner: "Google"
      depends_on_past: False
      start_date: '2021-03-01'
    max_active_runs: 1
    schedule_interval: "@daily"
    catchup: False
    default_view: graph

  tasks:
    - operator: "KubernetesPodOperator"
      description: "Run CSV transform within kubernetes pod"
      args:
        task_id: "transform_csv"
        startup_timeout_seconds: 600
        name: "deaths"
        namespace: "default"
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: cloud.google.com/gke-nodepool
                      operator: In
                      values:
                        - "pool-e2-standard-4"
        image_pull_policy: "Always"
        image: "{{ var.json.covid19_jhu_csse.container_registry.run_csv_transform_kub }}"
        env_vars:
          SOURCE_URL: "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv"
          SOURCE_FILE: "files/data.csv"
          TARGET_FILE: "files/data_output.csv"
          CHUNKSIZE: "1000000"
          TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          TARGET_GCS_PATH: "data/covid19_jhu_csse/recovered_cases/data_output.csv"
          PIPELINE_NAME: "recovered_cases"
          RENAME_MAPPINGS: >-
           {"Province/State": "province_state","Country/Region": "country_region","Lat": "latitude","Long": "longitude"}
          CSV_HEADERS: >-
            ["province_state","country_region","latitude","longitude","location_geom"]
        resources:
          limit_memory: "3G"
          limit_cpu: "2"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load CSV data to a BigQuery table"
      args:
        task_id: "load_to_bq"
        bucket: "{{ var.value.composer_bucket }}"
        source_objects: ["data/covid19_jhu_csse/recovered_cases/data_output.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "covid19_jhu_csse.recovered_cases"
        skip_leading_rows: 1
        write_disposition: "WRITE_TRUNCATE"

        schema_fields:
          - name: "province_state"
            type: "STRING"
            mode: "NULLABLE"
          - name: "country_region"
            type: "STRING"
            mode: "NULLABLE"
          - name: "latitude"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "longitude"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "location_geom"
            type: "GEOGRAPHY"
            mode: "NULLABLE"

  graph_paths:
    - "transform_csv >> load_to_bq"
