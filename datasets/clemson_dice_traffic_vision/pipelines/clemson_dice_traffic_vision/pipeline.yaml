# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:
  - type: bigquery_table
    table_id: "traffic_vision"
    description: "Clemson DICE Traffic Vision"

dag:
  airflow_version: 2
  initialize:
    dag_id: clemson_dice_traffic_vision
    default_args:
      owner: "Google"
      depends_on_past: False
      start_date: '2021-03-01'
    max_active_runs: 1
    schedule_interval: "@once"
    catchup: False
    default_view: graph

  tasks:
    # - operator: "GoogleCloudStorageToGoogleCloudStorageOperator"
    #   description: "Task to copy over to the pod, the source data and structure from GCS"
    #   args:
    #     task_id: "transfer_zip_files"
    #     source_bucket: "gcs-public-data-trafficvision"
    #     source_object: "*.tar.gz"
    #     destination_bucket: "{{ var.value.composer_bucket }}"
    #     destination_object: "data/trafficvision/files/"
    #     move_object: False
    #     replace: False
    - operator: "KubernetesPodOperator"
      description: "Run CSV transform within kubernetes pod"
      args:
        task_id: "transform_clemson_dice_data"
        startup_timeout_seconds: 600
        name: "transform_clemson_dice_data"
        namespace: "composer"
        service_account_name: "datasets"
        image_pull_policy: "Always"
        image: "{{ var.json.clemson_dice_traffic_vision.container_registry.run_csv_transform_kub }}"
        env_vars:
          SOURCE_URL_GCS: "gs://{{ var.value.composer_bucket }}/data/trafficvision/files"
          SOURCE_FILE_BATCH_LENGTH: "2000"
          TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          TARGET_GCS_PATH: "data/trafficvision/load_files"
          TARGET_ROOT_PATH: "data/trafficvision"
          TARGET_SOURCE_FOLDER: "files"
          TARGET_UNPACK_FOLDER: "unpack"
          TARGET_LOAD_FOLDER: "load_files"
          TARGET_BATCH_FOLDER: "batch_metadata"
          PROJECT_ID: "{{ var.value.gcp_project }}"
          PIPELINE_NAME: "generate_batch_metadata_files"
        resources:
          request_memory: "24G"
          request_cpu: "2"
          request_ephemeral_storage: "10G"
    - operator: "KubernetesPodOperator"
      description: "Run CSV transform within kubernetes pod"
      args:
        task_id: "run_batch_data_group_ord_1"
        startup_timeout_seconds: 600
        name: "run_batch_data_group_ord_1"
        namespace: "composer"
        service_account_name: "datasets"
        image_pull_policy: "Always"
        image: "{{ var.json.clemson_dice_traffic_vision.container_registry.run_csv_transform_kub }}"
        env_vars:
          SOURCE_URL_GCS: "gs://{{ var.value.composer_bucket }}/data/trafficvision/files"
          TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          TARGET_GCS_PATH: "data/trafficvision"
          TARGET_ROOT_PATH: "trafficvision"
          TARGET_SOURCE_FOLDER: "files"
          TARGET_UNPACK_FOLDER: "unpack"
          TARGET_LOAD_FOLDER: "load_files"
          TARGET_BATCH_FOLDER: "batch_metadata"
          PROJECT_ID: "{{ var.value.gcp_project }}"
          PIPELINE_NAME: "run_batch_data"
          BATCH_GROUP_SIZE: "2"
          BATCH_ORDINAL: "1"
        resources:
          request_memory: "24G"
          request_cpu: "2"
          request_ephemeral_storage: "10G"
    - operator: "KubernetesPodOperator"
      description: "Run CSV transform within kubernetes pod"
      args:
        task_id: "run_batch_data_group_ord_2"
        startup_timeout_seconds: 600
        name: "run_batch_data_group_ord_2"
        namespace: "composer"
        service_account_name: "datasets"
        image_pull_policy: "Always"
        image: "{{ var.json.clemson_dice_traffic_vision.container_registry.run_csv_transform_kub }}"
        env_vars:
          SOURCE_URL_GCS: "gs://{{ var.value.composer_bucket }}/data/trafficvision/files"
          TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          TARGET_GCS_PATH: "data/trafficvision"
          TARGET_ROOT_PATH: "trafficvision"
          TARGET_SOURCE_FOLDER: "files"
          TARGET_UNPACK_FOLDER: "unpack"
          TARGET_LOAD_FOLDER: "load_files"
          TARGET_BATCH_FOLDER: "batch_metadata"
          PROJECT_ID: "{{ var.value.gcp_project }}"
          PIPELINE_NAME: "run_batch_data"
          BATCH_GROUP_SIZE: "2"
          BATCH_ORDINAL: "2"
        resources:
          request_memory: "24G"
          request_cpu: "2"
          request_ephemeral_storage: "10G"
    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Load JSON metadata files to BQ"
      args:
        task_id: "load_json_metadata_to_bq"
        bucket: "{{ var.value.composer_bucket }}"  # Use what's been copied over
        source_objects: ["data/trafficvision/load_files/out*.log"]
        source_format: "NEWLINE_DELIMITED_JSON"
        destination_project_dataset_table: "clemson_dice.traffic_vision"
        write_disposition: "WRITE_TRUNCATE"
  graph_paths:
    - "transform_clemson_dice_data >> [ run_batch_data_group_ord_1, run_batch_data_group_ord_2 ] >> load_json_metadata_to_bq"
    # - "transfer_zip_files >> transform_clemson_dice_data >> [ run_batch_data_group_ord_1, run_batch_data_group_ord_2 ] >> load_json_metadata_to_bq"
