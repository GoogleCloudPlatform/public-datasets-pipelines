# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:
  - type: bigquery_table
    table_id: "traffic_vision"
    description: "Clemson DICE Traffic Vision"

dag:
  airflow_version: 2
  initialize:
    dag_id: clemson_dice_traffic_vision
    default_args:
      owner: "Google"
      depends_on_past: False
      start_date: '2021-03-01'
    max_active_runs: 1
    schedule_interval: "@once"
    catchup: False
    default_view: graph

  tasks:
    - operator: "GKECreateClusterOperator"
      args:
        task_id: "create_cluster"
        project_id: "{{ var.value.gcp_project }}"
        location: "us-central1-c"
        body:
          name: clemson_dice_traffic_vision
          initial_node_count: 2
          network: "{{ var.value.vpc_network }}"
          node_config:
            machine_type: e2-standard-8
            oauth_scopes:
              - https://www.googleapis.com/auth/devstorage.read_write
              - https://www.googleapis.com/auth/cloud-platform
    - operator: "GKEStartPodOperator"
      description: "Run CSV transform within kubernetes pod"
      args:
        task_id: "green_trips"
        startup_timeout_seconds: 600
        name: "load_tlc_green_trips"
        namespace: "default"
        project_id: "{{ var.value.gcp_project }}"
        location: "us-central1-c"
        cluster_name: new-york-taxi-trips
        image_pull_policy: "Always"
        image: "{{ var.json.new_york_taxi_trips.container_registry.run_csv_transform_kub }}"
        env_vars:
          SOURCE_URL_GCS: "gs://gcs-public-data-trafficvision"
          SOURCE_FILE_BATCH_LENGTH: "1000"
          CHUNKSIZE: "500000"
          TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          TARGET_GCS_ROOT_PATH: "data/trafficvision"
          TARGET_GCS_SOURCE_FOLDER: "files"
          TARGET_GCS_UNPACK_FOLDER: "unpack"
          TARGET_GCS_LOAD_FOLDER: "load_files"
          PROJECT_ID: "{{ var.value.gcp_project }}"
          DATASET_ID: "clemson_dice"
          TABLE_ID: "traffic_vision"
        resources:
          request_memory: "12G"
          request_cpu: "1"
          request_ephemeral_storage: "16G"
    - operator: "GKEDeleteClusterOperator"
      args:
        task_id: "delete_cluster"
        project_id: "{{ var.value.gcp_project }}"
        location: "us-central1-c"
        name: clemson_dice_traffic_vision

    # - operator: "CloudDataTransferServiceGCSToGCSOperator"
    #   description: "Copy .tar.gz source files to public bucket"
    #   args:
    #     task_id: transfer_zip_files
    #     timeout: 43200  # 12 hours
    #     retries: 0
    #     wait: True
    #     project_id: "bigquery-public-data-dev"
    #     source_bucket: "gcs-public-data-trafficvision"
    #     destination_bucket: "{{ var.value.composer_bucket }}"
    #     destination_path: "/data/trafficvision/files"
    #     transfer_options:
    #       overwriteWhen: ALWAYS
    #       deleteObjectsUniqueInSink: True
    # - operator: "BashOperator"
    #   description: "Task to transform the copied data files"
    #   args:
    #     task_id: "transform_files"
    #     bash_command: |
    #         mkdir -p "$WORKING_DIR/unpack" ;
    #         mkdir -p "$WORKING_DIR/load_files" ;
    #         cnt=0 ;
    #         for f in $WORKING_DIR/files/*.tar.gz;
    #           do let cnt=cnt+1
    #             rem=$((cnt % 5))
    #             tar -xzf "$f" -C "$WORKING_DIR/unpack" ; \
    #             guid="$(basename ${f/.tar.gz/})" ; \
    #             #echo $guid ; \
    #             sedval='s/{\"frame\"/{"id": \"'$guid'\"\, "frame"/' ; \
    #             sed "$sedval" $WORKING_DIR/unpack/$guid/out.log > $WORKING_DIR/load_files/out"$guid".log ;
    #             if [ $rem == "0" ]; then
    #                 echo "completed $cnt files "
    #             fi
    #         done
    #         echo $cnt
    #     env:
    #       WORKING_DIR: /home/airflow/gcs/data/trafficvision
    # - operator: "GoogleCloudStorageToBigQueryOperator"
    #   description: "Load JSON metadata files to BQ"
    #   args:
    #     task_id: "load_json_metadata_to_bq"
    #     bucket: "{{ var.value.composer_bucket }}"  # Use what's been copied over
    #     source_objects: ["data/trafficvision/load_files/out*.log"]
    #     source_format: "NEWLINE_DELIMITED_JSON"
    #     destination_project_dataset_table: "clemson_dice.traffic_vision"
    #     write_disposition: "WRITE_TRUNCATE"

  graph_paths:
    - "create_cluster >> load_clemson_dice_data >> delete_cluster"
    # - "transfer_zip_files >> transform_files >> load_json_metadata_to_bq"
