# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:
  # A list of GCP resources that are unique and specific to your pipeline.
  #
  # The currently supported resources are shown below. Use only the resources
  # needed by your pipeline, and delete the rest of the examples.
  #
  # We will keep adding to the list below to support more Google Cloud resources
  # over time. If a resource you need isn't supported, please file an issue on
  # the repository.

  - type: bigquery_table
    # A Google BigQuery table to store your data. Requires a `bigquery_dataset`
    # to be specified in the config (i.e. `dataset.yaml) for the dataset that
    # this pipeline belongs in.
    #
    # Required Properties:
    #   table_id
    table_id: inpatient_charges_2015

dag:
  # The DAG acronym stands for directed acyclic graph. This block represents
  # your data pipeline along with every property and configuration it needs to
  # onboard your data.
  initialize:
    dag_id: inpatient_charges_2015
    default_args:
      owner: "Google"

      # When set to True, keeps a task from getting triggered if the previous schedule for the task hasnâ€™t succeeded
      depends_on_past: False
      start_date: '2021-03-01'
    max_active_runs: 1
    schedule_interval: "@once"  # run once a week at Sunday 12am
    catchup: False
    default_view: graph

  tasks:
    # This is where you specify the tasks (a.k.a. processes) that your data
    # pipeline will run to onboard the data.
    #
    # As the examples below will show, every task must be represented by an
    # Airflow operator. The list of suported operators are listed in
    #
    #   scripts/dag_imports.json
    #
    # If an operator you need isn't supported, please file an issue on the
    # repository.
    #
    # Use the YAML list syntax in this block to specify every task for your
    # pipeline.

    - operator: "BashOperator"
      description: ~
      args:
        task_id: "copy_csv_file_to_gcs"
        bash_command: |
          echo $airflow_data_folder
          echo $csv_source_url
          mkdir -p $airflow_data_folder/cms_medicare/inpatient_charges_2015
          curl -o $airflow_data_folder/cms_medicare/inpatient_charges_2015/inpatient-data-2015-{{ ds }}.zip -L $csv_source_url
          cd $airflow_data_folder/cms_medicare/inpatient_charges_2015 && unzip inpatient-data-2015-{{ ds }}.zip
        env:
          csv_source_url: "https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Downloads/Inpatient_Data_2015_CSV.zip"
          airflow_data_folder: "{{ var.json.shared.airflow_data_folder }}"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      # Initializes GCS to BQ task for the DAG. This operator is used to load a
      # CSV file from GCS into a BigQuery table.

      # Task description
      description: "Task to load CSV data to a BigQuery table"

      args:
        # Arguments supported by this operator:
        # http://airflow.apache.org/docs/apache-airflow/1.10.14/howto/operator/gcp/gcs.html#googlecloudstoragetobigqueryoperator

        task_id: "load_csv_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.json.shared.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/cms_medicare/inpatient_charges_2015/inpatient-data-2015-{{ ds }}/Medicare_Provider_Charge_Inpatient_DRGALL_FY2015.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "cms_medicare.inpatient_charges_2015"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: provider_id
            type: string
            description: The CMS Certification Number (CCN) of the provider billing for outpatient
              hospital services
            mode: required
          - name: provider_name
            type: string
            description: The name of the provider
            mode: nullable
          - name: provider_street_address
            type: string
            description: The street address in which the provider is physically located
            mode: nullable
          - name: provider_city
            type: string
            description: The city in which the provider is physically located
            mode: nullable
          - name: provider_state
            type: string
            description: The state in which the provider is physically located
            mode: nullable
          - name: provider_zipcode
            type: integer
            description: The zip code in which the provider is physically located
            mode: nullable
          - name: drg_definition
            type: string
            description: The code and description identifying the MS-DRG. MS-DRGs are a classification
              system that groups similar clinical conditions (diagnoses) and the procedures
              furnished by the hospital during the stay
            mode: required
          - name: hospital_referral_region_description
            type: string
            description: The Hospital Referral Region (HRR) in which the provider is physically
              located
            mode: nullable
          - name: total_discharges
            type: integer
            description: The number of discharges billed by the provider for inpatient hospital
              services
            mode: nullable
          - name: average_covered_charges
            type: float
            description: The provider's average charge for services covered by Medicare for
              all discharges in the MS-DRG. These will vary from hospital to hospital because
              of differences in hospital charge structures
            mode: nullable
          - name: average_total_payments
            type: float
            description: The average total payments to all providers for the MS-DRG including
              the MSDRG amount, teaching, disproportionate share, capital, and outlier payments
              for all cases. Also included 5 in average total payments are co-payment and deductible
              amounts that the patient is responsible for and any additional payments by third
              parties for coordination of benefits
            mode: nullable
          - name: average_medicare_payments
            type: float
            description: The average amount that Medicare pays to the provider for Medicare's
              share of the MS-DRG. Average Medicare payment amounts include the MS-DRG amount,
              teaching, disproportionate share, capital, and outlier payments for all cases.
              Medicare payments DO NOT include beneficiary co-payments and deductible amounts
              nor any additional payments from third parties for coordination of benefits.
            mode: nullable

  graph_paths:
    # This is where you specify the relationships (i.e. directed paths/edges)
    # among the tasks specified above. Use the bitshift operator to define the
    # relationships and the `task_id` value above to represent tasks.
    #
    # For more info, see
    # https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html#setting-up-dependencies
    - "copy_csv_file_to_gcs >> load_csv_to_bq"