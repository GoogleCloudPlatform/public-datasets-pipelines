{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "SF 311 - Prediction",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i8lsRFe2lu5"
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial, we train a LSTM model to predict the number of non-emergency calls in San Francisco based on the call category (e.g. <em>Tree Maintenance</em>)."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "# Installing the required libraries:\n",
    "!pip install matplotlib pandas scikit-learn tensorflow tqdm pyarrow google-cloud-bigquery google-cloud-bigquery-storage"
   ],
   "metadata": {
    "id": "cFeLh7K7KI6B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6_jTxerkMtkg"
   },
   "source": [
    "# Python Builtin Libraries\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Third Party Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configurations\n",
    "%matplotlib inline\n",
    "%reload_ext google.cloud.bigquery"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authentication\n",
    "In order to run this tutorial successfully, we need to be authenticated first. \n",
    "\n",
    "Depending on where we are running this notebook, the authentication steps may vary:\n",
    "\n",
    "| Runner      | Authentiction Steps |\n",
    "| ----------- | ----------- |\n",
    "| Local Computer      |  Use a service account, or run the following command: <br><br>`gcloud auth login`    |\n",
    "| Colab   | Run the following python code and follow the instructions: <br><br>`from google.colab import auth` <br> `auth.authenticate_user()     `    |\n",
    "| Vertext AI (Workbench)   | Authentication is provided by Workbench       |"
   ],
   "metadata": {
    "id": "2x4wG61omjBQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "  from google.colab import auth\n",
    "  print('Authenticating in Colab')\n",
    "  auth.authenticate_user()\n",
    "  print('Authenticated')\n",
    "except:\n",
    "    print('This notebook is not running on Colab.')\n",
    "    print('Please make sure to follow the authentication steps.')"
   ],
   "metadata": {
    "id": "i7aszhgnkxuv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configurations\n",
    "\n",
    "Let's make sure we enter the name of our GCP project in the next cell."
   ],
   "metadata": {
    "id": "N7qEYK98Nx89"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "So_ed4wf0lKu"
   },
   "source": [
    "# ENTER THE GCP PROJECT HERE\n",
    "GCP_PROJECT = ''\n",
    "if not GCP_PROJECT:\n",
    "    GCP_PROJECT = input()\n",
    "print(f'GCP PROJECT is set to {GCP_PROJECT}')\n",
    "\n",
    "# Just a list of few existing categories:\n",
    "SAMPLE_CATEGORIES = ['Street and Sidewalk Cleaning', 'Encampments',\n",
    "                     'Litter Receptacles', 'Tree Maintenance',\n",
    "                     'Sewer Issues', 'Muni Service Feedback']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXIitTXv7IEu"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBLKdccfZzKu"
   },
   "source": [
    "### Query the Data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%bigquery dataframe --project $GCP_PROJECT \n",
    "WITH\n",
    "  intermediate1 AS (\n",
    "  SELECT\n",
    "    DATE_DIFF(created_date, CURRENT_TIMESTAMP(), DAY) AS days_ago,\n",
    "    category,\n",
    "  FROM\n",
    "    `bigquery-public-data.san_francisco_311.311_service_requests`\n",
    "  WHERE\n",
    "    latitude > 35\n",
    "    AND longitude < -122 ),\n",
    "  intermediate2 AS (\n",
    "  SELECT\n",
    "    DIV(days_ago - MIN(days_ago) OVER(), 7) AS week,\n",
    "    category\n",
    "  FROM\n",
    "    intermediate1)\n",
    "SELECT\n",
    "  category,\n",
    "  week,\n",
    "  COUNT(*) AS count\n",
    "FROM\n",
    "  intermediate2\n",
    "GROUP BY\n",
    "  category,\n",
    "  week;"
   ],
   "metadata": {
    "id": "1LJ3CTKyGP9c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I8mnYhOBsnr"
   },
   "source": [
    "### Check the Dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "84lvVNg8odvS"
   },
   "source": [
    "print(dataframe.shape)\n",
    "dataframe.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5VgmGUI_LBbT"
   },
   "source": [
    "category = SAMPLE_CATEGORIES[3]\n",
    "print(f'Selected category: {category}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2Xhrs_BaDex"
   },
   "source": [
    "### Prepare the Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the selected category above, we create a weekly time series based on number of calls that category has received in a given week:"
   ],
   "metadata": {
    "id": "slkY8m4vrf3T"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GnTwrINN_1di"
   },
   "source": [
    "category_df = dataframe[dataframe['category'] == category]\n",
    "max_week = category_df['week'].max()\n",
    "min_week = category_df['week'].min()\n",
    "# Some weeks may have no data, in which case we will set the value to zero.\n",
    "category_timeseries = np.zeros(max_week - min_week + 1)\n",
    "for index, row in category_df.iterrows():\n",
    "    category_timeseries[row['week'] - min_week] = row['count']\n",
    "category_timeseries = category_timeseries.reshape(-1, 1)\n",
    "print(f'Length of the time series: {len(category_timeseries)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlSMpnY_TF0X"
   },
   "source": [
    "We then normalize the time series to ensure all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mVEW159yByz5"
   },
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "nrm_timeseries = scaler.fit_transform(category_timeseries)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnymuBeST_jZ"
   },
   "source": [
    "We split the time series into two partitions for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hgAMYfVAS8Nc"
   },
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "train_size = int(len(nrm_timeseries) * TRAIN_RATIO)\n",
    "test_size = len(nrm_timeseries) - train_size\n",
    "train_timeseries = nrm_timeseries[0:train_size, :]\n",
    "test_timeseries = nrm_timeseries[train_size: len(nrm_timeseries), :]\n",
    "print(f'Training Size: {len(train_timeseries)}')\n",
    "print(f'Test Size: {len(test_timeseries)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW2bChBMVnZV"
   },
   "source": [
    "Next, we need to create an input matrix and an output vector from our normalized time series. Here is an example:\n",
    "\n",
    "Let's assume we have the following variables:\n",
    "```\n",
    "timeseries = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "width = 3\n",
    "```\n",
    "\n",
    "The matrix and the vector will then be:\n",
    "\n",
    "```\n",
    "matrix = [[0.1, 0.2, 0.3],\n",
    "          [0.2, 0.3, 0.4],\n",
    "          [0.3, 0.4, 0.5]]\n",
    "\n",
    "vector = [0.4, 0.5, 0.6]\n",
    "```\n",
    "\n",
    "The `width` can help us determine how many weeks in the time series we want to look back to make a prediction about the next week.\n",
    "\n",
    "In the example above, we want to predict a new entry by looking back at the previous 3 weeks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A7QuYWUWDY2l"
   },
   "source": [
    "def create_input_output_datasets(timeseries, width=5):\n",
    "\t\"\"\"\n",
    "\tGiven a time series, create a matrix as the input\n",
    "\tfor the training and a vector as the output.\n",
    "\t\"\"\"\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(timeseries) - width - 1):\n",
    "\t\tdataX.append(timeseries[i: (i + width), 0])\n",
    "\t\tdataY.append(timeseries[i + width, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef60fHXNYXPK"
   },
   "source": [
    "Let's create the input and output for both training and test time series."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qx5TTnttGEDv"
   },
   "source": [
    "weeks_back = 3 # Determining how far back we want to look at the data to make prediction about the next entry\n",
    "trainX, trainY = create_input_output_datasets(train_timeseries, weeks_back)\n",
    "testX, testY = create_input_output_datasets(test_timeseries, weeks_back)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVkOVYFqYwiZ"
   },
   "source": [
    "Finally, we need to reshape input with the  `[samples, time_steps, features]` dimensions where `time_steps = 1`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mibI4CpWGENY"
   },
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQUXlrp4NroU"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "Now that our data is prepared, we can build and train the model. Here is a simple LSTM model.\n",
    "\n",
    "We should not need a GPU to train this model since our training dataset is not too big.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9rIgZnLlS93y"
   },
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(10, input_shape=(1, weeks_back)))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=5, batch_size=1, verbose=1);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJbWtW-BeHrl"
   },
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6KE1e09b3LJ"
   },
   "source": [
    "With the trained model, we can create predictions for both training and test datasets and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lqI_CzHD85kU"
   },
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxQahjTEcDm5"
   },
   "source": [
    "Since we previously normalized our time series, we need to inverse that step now that we have prediction results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G_yFFPlAFojf"
   },
   "source": [
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLr1lyPIcSVa"
   },
   "source": [
    "Let's evaluate the results for both training and test datasets. We use the Root Mean Squared Error for both cases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WQ4c8C-4FikP"
   },
   "source": [
    "trainScore = np.sqrt(metrics.mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "testScore = np.sqrt(metrics.mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FTyz6HfdGf9"
   },
   "source": [
    "To plot the results, let's first shift both predictions to their right spot on the X-axis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3z0euEOFFiuP"
   },
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(nrm_timeseries)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[weeks_back: len(trainPredict) + weeks_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(nrm_timeseries)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict) + (weeks_back * 2) + 1: len(nrm_timeseries) - 1, :] = testPredict"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INaVbZTIdaag"
   },
   "source": [
    "Finally, we can plot the results of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gPSBSXt4Ftor"
   },
   "source": [
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.plot(category_timeseries, label='Actual Data')\n",
    "plt.plot(trainPredictPlot, label='Training - Predicted')\n",
    "plt.plot(testPredictPlot, label='Test - Predicted')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(f'Timeseries plot for \"{category}\"')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result seems reasonable. As a practice, you may want to try another categoty or change the `weeks_back` parameter and see the performance of different models."
   ],
   "metadata": {
    "id": "QDyoiI6PsqaC"
   }
  }
 ]
}