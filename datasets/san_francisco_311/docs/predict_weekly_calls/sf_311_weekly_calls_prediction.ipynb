{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i8lsRFe2lu5"
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial, we train a LSTM model to predict the number of non-emergency calls in San Francisco based on the call category (e.g. <em>Tree Maintenance</em>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFeLh7K7KI6B"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "# Installing the required libraries:\n",
    "!pip install matplotlib pandas scikit-learn tensorflow pyarrow tqdm\n",
    "!pip install google-cloud-bigquery google-cloud-bigquery-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_jTxerkMtkg"
   },
   "outputs": [],
   "source": [
    "# Python Builtin Libraries\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# Third Party Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "# Configurations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x4wG61omjBQ"
   },
   "source": [
    "### Authentication\n",
    "In order to run this tutorial successfully, we need to be authenticated first. \n",
    "\n",
    "Depending on where we are running this notebook, the authentication steps may vary:\n",
    "\n",
    "| Runner      | Authentiction Steps |\n",
    "| ----------- | ----------- |\n",
    "| Local Computer      |  Use a service account, or run the following command: <br><br>`gcloud auth login`    |\n",
    "| Colab   | Run the following python code and follow the instructions: <br><br>`from google.colab import auth` <br> `auth.authenticate_user()     `    |\n",
    "| Vertext AI (Workbench)   | Authentication is provided by Workbench       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7aszhgnkxuv"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "\n",
    "    print(\"Authenticating in Colab\")\n",
    "    auth.authenticate_user()\n",
    "    print(\"Authenticated\")\n",
    "except:  # noqa\n",
    "    print(\"This notebook is not running on Colab.\")\n",
    "    print(\"Please make sure to follow the authentication steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7qEYK98Nx89"
   },
   "source": [
    "### Configurations\n",
    "\n",
    "Let's make sure we enter the name of our GCP project in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "So_ed4wf0lKu"
   },
   "outputs": [],
   "source": [
    "# ENTER THE GCP PROJECT HERE\n",
    "gcp_project = \"endromodal\"\n",
    "print(f\"gcp_project is set to {gcp_project}\")\n",
    "\n",
    "# Just a list of few existing categories:\n",
    "sample_categories = [\n",
    "    \"Street and Sidewalk Cleaning\",\n",
    "    \"Encampments\",\n",
    "    \"Litter Receptacles\",\n",
    "    \"Tree Maintenance\",\n",
    "    \"Sewer Issues\",\n",
    "    \"Muni Service Feedback\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXIitTXv7IEu"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBLKdccfZzKu"
   },
   "source": [
    "### Query the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH\n",
    "  intermediate1 AS (\n",
    "  SELECT\n",
    "    DATE_DIFF(created_date, CURRENT_TIMESTAMP(), DAY) AS days_ago,\n",
    "    category,\n",
    "  FROM\n",
    "    `bigquery-public-data.san_francisco_311.311_service_requests`\n",
    "  WHERE\n",
    "    latitude > 35\n",
    "    AND longitude < -122 ),\n",
    "  intermediate2 AS (\n",
    "  SELECT\n",
    "    DIV(days_ago - MIN(days_ago) OVER(), 7) AS week,\n",
    "    category\n",
    "  FROM\n",
    "    intermediate1)\n",
    "SELECT\n",
    "  category,\n",
    "  week,\n",
    "  COUNT(*) AS count\n",
    "FROM\n",
    "  intermediate2\n",
    "GROUP BY\n",
    "  category,\n",
    "  week;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client(project=gcp_project)\n",
    "dataframe = bqclient.query(query).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I8mnYhOBsnr"
   },
   "source": [
    "### Check the Dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84lvVNg8odvS"
   },
   "outputs": [],
   "source": [
    "print(dataframe.shape)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VgmGUI_LBbT"
   },
   "outputs": [],
   "source": [
    "category = sample_categories[3]\n",
    "print(f\"Selected category: {category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2Xhrs_BaDex"
   },
   "source": [
    "### Prepare the Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slkY8m4vrf3T"
   },
   "source": [
    "For the selected category above, we create a weekly time series based on number of calls that category has received in a given week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnTwrINN_1di"
   },
   "outputs": [],
   "source": [
    "category_df = dataframe[dataframe[\"category\"] == category]\n",
    "max_week = category_df[\"week\"].max()\n",
    "min_week = category_df[\"week\"].min()\n",
    "# Some weeks may have no data, in which case we will set the value to zero.\n",
    "category_timeseries = np.zeros(max_week - min_week + 1)\n",
    "for index, row in category_df.iterrows():\n",
    "    category_timeseries[row[\"week\"] - min_week] = row[\"count\"]\n",
    "category_timeseries = category_timeseries.reshape(-1, 1)\n",
    "print(f\"Length of the time series: {len(category_timeseries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlSMpnY_TF0X"
   },
   "source": [
    "We then normalize the time series to ensure all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVEW159yByz5"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "nrm_timeseries = scaler.fit_transform(category_timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnymuBeST_jZ"
   },
   "source": [
    "We split the time series into two partitions for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgAMYfVAS8Nc"
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train_size = int(len(nrm_timeseries) * train_ratio)\n",
    "test_size = len(nrm_timeseries) - train_size\n",
    "train_timeseries = nrm_timeseries[0:train_size, :]\n",
    "\n",
    "test_timeseries = nrm_timeseries[train_size: len(nrm_timeseries), :]\n",
    "print(f\"Training Size: {len(train_timeseries)}\")\n",
    "print(f\"Test Size: {len(test_timeseries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW2bChBMVnZV"
   },
   "source": [
    "Next, we need to create an input matrix and an output vector from our normalized time series. Here is an example:\n",
    "\n",
    "Let's assume we have the following variables:\n",
    "```\n",
    "timeseries = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "width = 3\n",
    "```\n",
    "\n",
    "The matrix and the vector will then be:\n",
    "\n",
    "```\n",
    "matrix = [[0.1, 0.2, 0.3],\n",
    "          [0.2, 0.3, 0.4],\n",
    "          [0.3, 0.4, 0.5]]\n",
    "\n",
    "vector = [0.4, 0.5, 0.6]\n",
    "```\n",
    "\n",
    "The `width` can help us determine how many weeks in the time series we want to look back to make a prediction about the next week.\n",
    "\n",
    "In the example above, we want to predict a new entry by looking back at the previous 3 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7QuYWUWDY2l"
   },
   "outputs": [],
   "source": [
    "def create_input_output_datasets(timeseries, width=5):\n",
    "    \"\"\"\n",
    "    Given a time series, create a matrix as the input\n",
    "    for the training and a vector as the output.\n",
    "    \"\"\"\n",
    "    data_x, data_y = [], []\n",
    "    for i in range(len(timeseries) - width - 1):\n",
    "        data_x.append(timeseries[i: (i + width), 0])\n",
    "        data_y.append(timeseries[i + width, 0])\n",
    "    return np.array(data_x), np.array(data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef60fHXNYXPK"
   },
   "source": [
    "Let's create the input and output for both training and test time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qx5TTnttGEDv"
   },
   "outputs": [],
   "source": [
    "# Determining how far back we want to look at\n",
    "# the data to make prediction about the next entry\n",
    "weeks_back = 3\n",
    "\n",
    "train_x, train_y = create_input_output_datasets(train_timeseries, weeks_back)\n",
    "test_x, test_y = create_input_output_datasets(test_timeseries, weeks_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVkOVYFqYwiZ"
   },
   "source": [
    "Finally, we need to reshape input with the  `[samples, time_steps, features]` dimensions where `time_steps = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mibI4CpWGENY"
   },
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQUXlrp4NroU"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "Now that our data is prepared, we can build and train the model. Here is a simple LSTM model.\n",
    "\n",
    "We should not need a GPU to train this model since our training dataset is not too big.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rIgZnLlS93y"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(10, input_shape=(1, weeks_back)))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "model.fit(train_x, train_y, epochs=5, batch_size=1, verbose=1);  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJbWtW-BeHrl"
   },
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6KE1e09b3LJ"
   },
   "source": [
    "With the trained model, we can create predictions for both training and test datasets and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqI_CzHD85kU"
   },
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_x)\n",
    "test_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxQahjTEcDm5"
   },
   "source": [
    "Since we previously normalized our time series, we need to inverse that step now that we have prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_yFFPlAFojf"
   },
   "outputs": [],
   "source": [
    "train_pred = scaler.inverse_transform(train_pred)\n",
    "train_y = scaler.inverse_transform([train_y])\n",
    "test_pred = scaler.inverse_transform(test_pred)\n",
    "test_y = scaler.inverse_transform([test_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLr1lyPIcSVa"
   },
   "source": [
    "Let's evaluate the results for both training and test datasets. We use the Root Mean Squared Error for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ4c8C-4FikP"
   },
   "outputs": [],
   "source": [
    "train_score = np.sqrt(metrics.mean_squared_error(train_y[0], train_pred[:, 0]))\n",
    "print(\"Train Score: %.2f RMSE\" % (train_score))\n",
    "\n",
    "test_score = np.sqrt(metrics.mean_squared_error(test_y[0], test_pred[:, 0]))\n",
    "print(\"Test Score: %.2f RMSE\" % (test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FTyz6HfdGf9"
   },
   "source": [
    "To plot the results, let's first shift both predictions to their right spot on the X-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3z0euEOFFiuP"
   },
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "train_pred_plot = np.empty_like(nrm_timeseries)\n",
    "train_pred_plot[:, :] = np.nan\n",
    "train_pred_plot[weeks_back: len(train_pred) + weeks_back, :] = train_pred\n",
    "\n",
    "# shift test predictions for plotting\n",
    "test_pred_plot = np.empty_like(nrm_timeseries)\n",
    "test_pred_plot[:, :] = np.nan\n",
    "test_pred_plot[\n",
    "    len(train_pred) + (weeks_back * 2) + 1: len(nrm_timeseries) - 1, :\n",
    "] = test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INaVbZTIdaag"
   },
   "source": [
    "Finally, we can plot the results of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPSBSXt4Ftor"
   },
   "outputs": [],
   "source": [
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "plt.plot(category_timeseries, label=\"Actual Data\")\n",
    "plt.plot(train_pred_plot, label=\"Training - Predicted\")\n",
    "plt.plot(test_pred_plot, label=\"Test - Predicted\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(f'Timeseries plot for \"{category}\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDyoiI6PsqaC"
   },
   "source": [
    "The result seems reasonable. As a practice, you may want to try another categoty or change the `weeks_back` parameter and see the performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SF 311 - Prediction",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
