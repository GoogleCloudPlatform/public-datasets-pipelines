# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:

  - type: bigquery_table
    # Required Properties:
    table_id: bikeshare_stations

    # Description of the table
    description: "Austin Bikeshare Stations table"

dag:
  airflow_version: 2
  initialize:
    dag_id: bikeshare_stations
    default_args:
      owner: "Google"

      # When set to True, keeps a task from getting triggered if the previous schedule for the task hasnâ€™t succeeded
      depends_on_past: False
      start_date: '2021-03-01'
    max_active_runs: 1
    schedule_interval: "@daily"  # runs everyday at 6am EST
    catchup: False
    default_view: graph

  tasks:
    - operator: "KubernetesPodOperator"

      # Task description
      description: "Run CSV transform within kubernetes pod"

      args:

        task_id: "austin_bikeshare_stations_transform_csv"

        # The name of the pod in which the task will run. This will be used (plus a random suffix) to generate a pod id
        name: "bikeshare_stations"
        namespace: "composer"
        service_account_name: "datasets"

        image_pull_policy: "Always"

        # Docker images will be built and pushed to GCR by default whenever the `scripts/generate_dag.py` is run. To skip building and pushing images, use the optional `--skip-builds` flag.
        image: "{{ var.json.austin_bikeshare.container_registry.run_csv_transform_kub }}"

        # Set the environment variables you need initialized in the container. Use these as input variables for the script your container is expected to perform.
        env_vars:
          SOURCE_URL: "https://data.austintexas.gov/api/views/qd73-bsdg/rows.csv"
          SOURCE_FILE: "files/data.csv"
          TARGET_FILE: "files/data_output.csv"
          TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          TARGET_GCS_PATH: "data/austin_bikeshare/bikeshare_stations/data_output.csv"
          PIPELINE_NAME: "bikeshare_stations"
          CSV_HEADERS: >-
            ["station_id","name","status","address","alternate_name","city_asset_number","property_type","number_of_docks","power_type","footprint_length","footprint_width","notes","council_district","modified_date"]
          RENAME_MAPPINGS: >-
            {"Kiosk ID": "station_id","Kiosk Name": "name","Kiosk Status": "status","Address": "address","Alternate Name": "alternate_name","City Asset Number": "city_asset_number","Property Type": "property_type","Number of Docks": "number_of_docks","Power Type": "power_type","Footprint Length": "footprint_length","Footprint Width": "footprint_width","Notes": "notes","Council District": "council_district","Modified Date": "modified_date"}

        # Set resource limits for the pod here. For resource units in Kubernetes, see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes
        resources:
          request_memory: "4G"
          request_cpu: "1"
          request_ephemeral_storage: "10G"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load CSV data to a BigQuery table"

      args:
        task_id: "load_austin_bikeshare_stations_to_bq"

        # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

        # The GCS object path for the CSV file
        source_objects: ["data/austin_bikeshare/bikeshare_stations/data_output.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "austin_bikeshare.bikeshare_stations"

        # Use this if your CSV file contains a header row
        skip_leading_rows: 1

        # How to write data to the table: overwrite, append, or write if empty
        # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

        # The BigQuery table schema based on the CSV file. For more info, see
        # https://cloud.google.com/bigquery/docs/schemas.
        # Always use snake_case and lowercase for column names, and be explicit,
        # i.e. specify modes for all columns.
        schema_fields:
          - name: "station_id"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "name"
            type: "STRING"
            mode: "NULLABLE"
          - name: "status"
            type: "STRING"
            mode: "NULLABLE"
          - name: "address"
            type: "STRING"
            mode: "NULLABLE"
          - name: "alternate_name"
            type: "STRING"
            mode: "NULLABLE"
          - name: "city_asset_number"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "property_type"
            type: "STRING"
            mode: "NULLABLE"
          - name: "number_of_docks"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "power_type"
            type: "STRING"
            mode: "NULLABLE"
          - name: "footprint_length"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "footprint_width"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "notes"
            type: "STRING"
            mode: "NULLABLE"
          - name: "council_district"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "modified_date"
            type: "TIMESTAMP"
            mode: "NULLABLE"

  graph_paths:
    - "austin_bikeshare_stations_transform_csv >> load_austin_bikeshare_stations_to_bq"
